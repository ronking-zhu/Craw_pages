import os
import re
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import unquote, urljoin
from tqdm import tqdm  # è¿›åº¦æ¡

# é…ç½®å‚æ•°
BASE_URL = "https://learn.lianglianglee.com/ä¸“æ /etcdå®æˆ˜è¯¾"
CATALOG_URL = "https://learn.lianglianglee.com/%e4%b8%93%e6%a0%8f/etcd%e5%ae%9e%e6%88%98%e8%af%be"  # ç›®å½•é¡µ
OUTPUT_DIR = "etcd_tutorial"  # ä¿å­˜ç›®å½•
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8'
}

def extract_links_from_catalog():
    """ä»ç›®å½•é¡µæå–æ‰€æœ‰æ–‡ç« é“¾æ¥"""
    try:
        print("ğŸ” æ­£åœ¨è·å–ç›®å½•é¡µå†…å®¹...")
        response = requests.get(CATALOG_URL, headers=HEADERS)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æŸ¥æ‰¾åŒ…å«æ‰€æœ‰æ–‡ç« çš„å®¹å™¨ - æ ¹æ®å®é™…ç»“æ„è°ƒæ•´
        catalog_section = soup.find('div', class_='catalog') or soup.find('ul', class_='posts-list')
        
        if not catalog_section:
            # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šå®¹å™¨ï¼Œå°è¯•é€šç”¨æ–¹æ³•
            all_links = soup.find_all('a', href=True)
            article_links = []
            for link in all_links:
                href = link['href']
                if "/etcd%e5%ae%9e%e6%88%98%e8%af%be/" in href and href.endswith('.md'):
                    article_links.append(href)
        else:
            # ä»ç›®å½•å®¹å™¨æå–ç²¾ç¡®é“¾æ¥
            article_links = [
                a['href'] for a in catalog_section.find_all('a', href=True) 
                if a['href'].endswith('.md')
            ]
        
        # å»é‡å¹¶ç”Ÿæˆå®Œæ•´URL
        article_links = list(set(article_links))
        full_urls = [urljoin(BASE_URL, link) for link in article_links]
        
        print(f"âœ… æ‰¾åˆ° {len(full_urls)} ç¯‡æ–‡ç« ")
        return full_urls
        
    except Exception as e:
        print(f"âŒ è·å–ç›®å½•å¤±è´¥: {str(e)}")
        return []

def download_article(url):
    """ä¸‹è½½å•ç¯‡æ–‡ç« å¹¶ä¿å­˜ä¸ºMarkdown"""
    try:
        # è·å–æ–‡ä»¶å
        filename = unquote(url.split('/')[-1]).replace('.md', '')
        safe_filename = re.sub(r'[\\/*?:"<>|]', "", filename) + ".md"
        
        # ä¸‹è½½å†…å®¹
        response = requests.get(url, headers=HEADERS)
        response.raise_for_status()
        
        # ç›´æ¥ä¿å­˜åŸå§‹Markdownå†…å®¹
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        filepath = os.path.join(OUTPUT_DIR, safe_filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(response.text)
        
        print(f"âœ… å·²ä¿å­˜: {safe_filename}")
        return True
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ [{url}]: {str(e)}")
        return False

def main():
    # è·å–æ‰€æœ‰æ–‡ç« é“¾æ¥
    article_urls = extract_links_from_catalog()
    
    if not article_urls:
        print("âš ï¸ æœªæ‰¾åˆ°æ–‡ç« é“¾æ¥ï¼Œè¯·æ£€æŸ¥ç›®å½•é¡µç»“æ„")
        return
    
    # æ‰¹é‡ä¸‹è½½
    print("â¬ å¼€å§‹ä¸‹è½½æ–‡ç« ...")
    success_count = 0
    
    for url in tqdm(article_urls, desc="ä¸‹è½½è¿›åº¦"):
        if download_article(url):
            success_count += 1
        time.sleep(3)  # è¯·æ±‚é—´éš”
    
    print(f"\nğŸ‰ ä¸‹è½½å®Œæˆ! æˆåŠŸ: {success_count}/{len(article_urls)}")

if __name__ == "__main__":
    main()